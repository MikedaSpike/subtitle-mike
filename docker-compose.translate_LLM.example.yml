services:
  subtitle-mike:
    image: mikedaspike/subtitle-mike:latest
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    env_file:
      - .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - PYTHONWARNINGS=ignore
      - TRANSLATE_TO_LANG=nl # Examples: 'nl' for Dutch, 'fr' for French, 'de' for German
      - LLM_URL=http://ollama:11434
      - LLM_MODEL=llama3.1
    volumes:
      - /path/to/your/media:/data
      - ./models/huggingface:/root/.cache/huggingface
      - ./models/whisper:/root/.cache/whisper
      - ./models/torch:/root/.cache/torch
    depends_on:
      ollama:
        condition: service_healthy

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:5000/health\").read()' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10